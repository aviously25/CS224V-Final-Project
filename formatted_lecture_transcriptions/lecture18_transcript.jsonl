{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Language Models and Data Centricity", "content": "I'm giving a guest lecture today on a variety of data centric language modeling topics. The reason why LMS do so well is because they're trained on incredibly large amounts of data. But studies show that language models struggle on niche entities. How can retrieval augmentation harm the language model's performance?", "block_metadata": {"start (ms)": 26035, "end (ms)": 872163}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > language modeling: the knowledge gap", "content": "As entity popularity goes down, language models do worse. The Internet data that we train on for language modeling is quite a precious resource. How can we more data efficiently train models? That's a very fundamental question to continue progress in building future language models.", "block_metadata": {"start (ms)": 872179, "end (ms)": 1083173}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Data Selection, Training, and Scalability", "content": "How do we select good data for pre training? In a way that will give us more capable, you know, better models. There's data, there's scaling, and then there's an attention to detail in the engineering.", "block_metadata": {"start (ms)": 1083309, "end (ms)": 1151015}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Machine Learning's Data Pipeline", "content": "Of the three pillars, data is the most unknown. There has been some work trying to open source, replicate data pipelines to train highly effective models. Are there simple, some principled approaches to classifying based on BI grabs like that would be cool?", "block_metadata": {"start (ms)": 1151055, "end (ms)": 1508577}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Why Algorithmic Data Selection is so Hard", "content": "Algorithmic data selection has remained so elusive and so hard. If you want to understand how data affects a language model, well, you have to remove data and train a model. The advantages here are no cost. Use publicly available models.", "block_metadata": {"start (ms)": 1508681, "end (ms)": 1853325}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Language Algorithms and the Web", "content": "Lambada: As an algorithm, it's very simple. All I do is I measure log losses on websites. I look at the websites that have the highest correlation between log likelihoods and downstream benchmarks. This turns out to be kind of an effective data selection algorithm.", "block_metadata": {"start (ms)": 1854825, "end (ms)": 2098215}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Pre-training and data centricity", "content": "Language effects are incredibly clear cut. English looks totally different from, you know, French or Spanish or any other language. We also see really clear differences in the difficulty of the web page, like the entropy of the website. Language and entropy are two really core ones.", "block_metadata": {"start (ms)": 2099835, "end (ms)": 2174545}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Pre-training and data selection in machine learning", "content": "How do we know that the research conclusions we reach are valid? One thing I'm excited about trying recently is to think about pre registration. Data selection for pre training is kind of the core algorithmic piece of that. The next part of the talk will talk about a very different thinking about data selection.", "block_metadata": {"start (ms)": 2175565, "end (ms)": 2440031}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Data efficiency and continued pre-training", "content": "We know that language models struggle when there's not enough data. This is going to become a problem as we start to either exhaust data or think about more niche domains. Here's a new more data efficient algorithm that lets me put the knowledge in, you know, these books into the parameters of the model.", "block_metadata": {"start (ms)": 2440223, "end (ms)": 2893121}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Language model evaluation", "content": "One of the core challenges is how do we do evaluation. The idea here is to take reading comprehension dataset in particular. As I generate more and more synthetic tokens from rephrasing, I actually see pretty predictable linear gains.", "block_metadata": {"start (ms)": 2893313, "end (ms)": 3407775}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Neuroanatomy 2.8 vs GPT4", "content": "This is the salmon colored line in this plot here. It scales much more nicely and more efficiently than just naively rephrasing. We can pretty handily beat GPT4, sort of closed book in doing QA. I think we can hopefully continue to scale with additional token inputs.", "block_metadata": {"start (ms)": 3407935, "end (ms)": 3595215}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > RAG vs LLAM: empirical evaluation", "content": "We built a RAG system that is incredibly accurate. Within the top eight articles, 99% of the time the right article is in there. Even against this strong baseline, if we apply continued pre training, we still see like 2 to 3% gains on this.", "block_metadata": {"start (ms)": 3596115, "end (ms)": 3692055}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Synthetic Data and its implications", "content": "Tatsu: We need to think very carefully about what is synthetic data buying us, what is it not buying us. Synthetic data is describing the relation between two entities that may not necessarily be directly in the document. We can use the same playbook to improve knowledge of language models on a domain.", "block_metadata": {"start (ms)": 3693515, "end (ms)": 3975365}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Pre-training and the data-", "content": "The central role of data and how we can algorithmically intervene on data in many ways. The place at which I'm going to intervene is on this kind of pre training style training. That's the place where we can deploy the most compute and that might have the greatest leverage.", "block_metadata": {"start (ms)": 3975865, "end (ms)": 4054065}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > How to Train Language Algorithms", "content": "Psychology major: How to present knowledge to models in a way that's more easily learnable. Do we need a new algorithm? Does data augmentation emulate how humans learn? How do we close the gap?", "block_metadata": {"start (ms)": 4054105, "end (ms)": 4173933}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Language models: Creativity", "content": "The goal of frontier researchers is like, to create new knowledge. But the biggest contributor to that is actually the alignment process. Continued pre training does, broadly speaking, make language models lose their capabilities.", "block_metadata": {"start (ms)": 4174029, "end (ms)": 4342923}}
{"document_title": "CS224V Lecture 18", "section_title": "CS224V Lecture 18 > Chapter Summaries > Artificial Intelligence: Creativity or Stupid?", "content": "If you don't vary the prompt, you'll often cap out. Creative is a double edged sword. In a way alignment is really focusing towards the average case. When there's a human in the loop, the game is very different.", "block_metadata": {"start (ms)": 4342979, "end (ms)": 5238965}}
