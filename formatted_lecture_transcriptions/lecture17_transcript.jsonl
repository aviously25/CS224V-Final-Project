{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > EMNLP: The Late Interaction Model", "content": "This week we're going to talk about information retrieval and entity disambiguation. It's all about making it accurate enough, but at the same time fast enough. There is a question of time optimization and space optimization.", "block_metadata": {"start (ms)": 309905, "end (ms)": 580055}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > Colbert V2 and the future of information retrieval", "content": "Colbert was developed at Stanford with Professor Zaharia Matei and his student Omar. It is contextualized late interaction. With Colbert V2, it takes it to a better performance and also at a lower latency. Can be used in many places from product search to question answering.", "block_metadata": {"start (ms)": 580635, "end (ms)": 922205}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > Machine Reading: The Matching Paradigm", "content": "The basic idea is that you have to figure out for each document, out of the gazillion documents you have, what is the match? There are two extreme matching paradigms. One is a classic cross encoder. The other is a very expensive neural network.", "block_metadata": {"start (ms)": 931915, "end (ms)": 1540755}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > Quantum Search 2, The second optimization", "content": "In a query, I have to find the top K matching documents from say 100 million documents. Ranking every document is too costly, so we use a cheaper method to narrow down the candidates. The second optimization is on space, and that is that it is a lot of space for the embedding.", "block_metadata": {"start (ms)": 1542095, "end (ms)": 1795185}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > Facebook AI similarity search: The", "content": "Face is the Facebook AI similarity search. It will find you the closest k vectors. The second stage is to run through the true pipeline, which is the late interaction pipeline. And it actually does a better job using this late interaction.", "block_metadata": {"start (ms)": 1796565, "end (ms)": 2191575}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > Quantum compression for 128-bit data", "content": "Omar Mate: The concept is to do residual compression. He says we represent each vector as a cluster ID and a 1 bit delta per dimension, 20 bytes per vector. Mate: This really has a significant impact on our, on information retrieval.", "block_metadata": {"start (ms)": 2191735, "end (ms)": 2561777}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > Information Retrieval 101", "content": "So what I want to talk about next is the. Oh, very. That doesn't work. I'm a little under the weather if you don't haven't figured it out yet. All right, so any questions about information retrieval?", "block_metadata": {"start (ms)": 2561931, "end (ms)": 2609155}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > Neural Network: Named Entity Disambiguation", "content": "The problem of named entity disambiguation can be separated into two parts. With Wikidata, every person has a unique id, regardless of how they are referred to in different countries. The key concept here is just add descriptions to the input. Once you add in the descriptions, then you can have a fixed entity.", "block_metadata": {"start (ms)": 2612575, "end (ms)": 3275263}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > Intelligence 6, Semantic Matching in Google NLP", "content": "By 2022 there is this paper by Amazon. This is an efficient, zero shot capable approach to end to end entity linking. They also add type information to the entities. It is a very useful module which we have very successfully in our projects.", "block_metadata": {"start (ms)": 3275359, "end (ms)": 3711435}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > Optimizing time for information retrieval", "content": "We talk about face, but like everything else, things keep improving. Instead of just memorizing input outputs, we are now retrieving information so that it returned the NED problem into a document matching problem. It is really amazing how fast we are progressing.", "block_metadata": {"start (ms)": 3711815, "end (ms)": 3862147}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > Microsoft's Search engine, Quadrant", "content": "Is it also using a very similar method to this Covert and mgt? Which one for Perplexity or modern reg system? Any other questions? No.", "block_metadata": {"start (ms)": 3862341, "end (ms)": 4036035}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > Projects in the end of the quarter", "content": "The course is kind of like 50% projects, 50% lectures. In the last class we will get together as a class and we want a one minute video from every group. We will do a poster after the one hour presentation. What you guys need to deliver by, you know, by two weeks plus two days.", "block_metadata": {"start (ms)": 4036775, "end (ms)": 4220145}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > How many pages should I write in a paper?", "content": "How many pages? So it's a very common question. I actually don't put a page limit on no minimum, no maximum. If you have very little to say but something very clear and crisp. Just take whatever pages that you need.", "block_metadata": {"start (ms)": 4220265, "end (ms)": 4309021}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > Pushing the related work in your PhD", "content": "When you describe related work, it is a very easy way to show off what you have done. I always recommend people to start with deciding on the demo and walking backwards. It is all about depth first. Whatever you cannot show in the demo, you can probably skip.", "block_metadata": {"start (ms)": 4309163, "end (ms)": 4506375}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > In terms of the agent,", "content": "As long as it has a system of LLM, you need nlp. So it doesn't have to be like, in a conversational format. And there are many, many projects possible, for sure.", "block_metadata": {"start (ms)": 4507915, "end (ms)": 4575475}}
{"document_title": "CS224V Lecture 17", "section_title": "CS224V Lecture 17 > Chapter Summaries > AI classes", "content": "In AI classes, we don't have final exams. What is the incentive for coming to class or even watching the videos? We will put the information about the videos, the posters, and the final projects online. See you on Wednesday.", "block_metadata": {"start (ms)": 4575815, "end (ms)": 4707955}}
